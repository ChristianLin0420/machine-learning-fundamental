# Optimization Algorithms

## 📌 Overview
Implement various optimization algorithms (SGD, Adam, RMSprop) to understand how different optimizers affect neural network training.

## 🧠 Key Concepts
- Stochastic Gradient Descent (SGD)
- Momentum and Nesterov Momentum
- Adam (Adaptive Moment Estimation)
- RMSprop and AdaGrad

## 🛠️ Implementation
- SGD with momentum implementation
- Adam optimizer with bias correction
- RMSprop adaptive learning rates
- Learning rate scheduling

## 📊 Results
- Convergence speed comparisons
- Loss landscape visualization
- Hyperparameter sensitivity analysis
- Optimizer performance on different datasets

## 📚 References
- [Adam Optimizer Paper](https://arxiv.org/abs/1412.6980)
- [An Overview of Gradient Descent Optimization](https://ruder.io/optimizing-gradient-descent/)
- [Why Momentum Really Works](https://distill.pub/2017/momentum/) 