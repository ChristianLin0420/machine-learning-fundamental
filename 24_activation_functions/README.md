# Activation Functions

## 📌 Overview
Implement and compare various activation functions (ReLU, Sigmoid, Tanh, etc.) and understand their impact on neural network training.

## 🧠 Key Concepts
- Sigmoid and Tanh Functions
- ReLU and its variants (Leaky ReLU, ELU)
- Swish and GELU
- Activation Function Properties

## 🛠️ Implementation
- Mathematical implementations of activations
- Derivative computations
- Gradient flow analysis
- Dead neuron detection

## 📊 Results
- Activation function comparisons
- Gradient saturation analysis
- Training speed comparisons
- Dead neuron statistics with ReLU

## 📚 References
- [Activation Functions Explained](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
- [Deep Learning Book - Chapter 6](https://www.deeplearningbook.org/)
- [ReLU Paper](https://proceedings.neurips.cc/paper/2010/file/4a47a0db6e1750e8d7a80c6c5e48b5be-Paper.pdf) 