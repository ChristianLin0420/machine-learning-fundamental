# Random Forest

## ğŸ“Œ Overview
Implement Random Forest algorithm combining multiple decision trees using bagging and random feature selection to improve accuracy and reduce overfitting.

## ğŸ§  Key Concepts
- Ensemble Learning and Bagging
- Bootstrap Sampling
- Random Feature Selection
- Out-of-Bag (OOB) Error Estimation

## ğŸ› ï¸ Implementation
- Bootstrap sample generation
- Random feature subset selection
- Multiple decision tree training
- Voting mechanism for predictions

## ğŸ“Š Results
- Comparison with single decision trees
- Feature importance aggregation
- OOB error vs test error analysis
- Performance metrics and confusion matrix

## ğŸ“š References
- [Random Forests Paper](https://link.springer.com/article/10.1023/A:1010933404324)
- [Ensemble Methods in Machine Learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)
- [Random Forest Guide](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) 