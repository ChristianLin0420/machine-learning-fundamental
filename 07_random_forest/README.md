# Random Forest

## 📌 Overview
Implement Random Forest algorithm combining multiple decision trees using bagging and random feature selection to improve accuracy and reduce overfitting.

## 🧠 Key Concepts
- Ensemble Learning and Bagging
- Bootstrap Sampling
- Random Feature Selection
- Out-of-Bag (OOB) Error Estimation

## 🛠️ Implementation
- Bootstrap sample generation
- Random feature subset selection
- Multiple decision tree training
- Voting mechanism for predictions

## 📊 Results
- Comparison with single decision trees
- Feature importance aggregation
- OOB error vs test error analysis
- Performance metrics and confusion matrix

## 📚 References
- [Random Forests Paper](https://link.springer.com/article/10.1023/A:1010933404324)
- [Ensemble Methods in Machine Learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)
- [Random Forest Guide](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) 