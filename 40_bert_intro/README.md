# Day 40 â€” BERT Intro (Encoder-only Transformers, MLM Pretraining)

## ğŸ“Œ Overview
A complete implementation of BERT from scratch, including the transformer encoder architecture and Masked Language Modeling (MLM) pretraining objective. This implementation demonstrates the core concepts behind bidirectional encoder representations.

## ğŸ—ï¸ Architecture

### Model Components
- **BERT Encoder**: Multi-layer transformer encoder with pre-layer normalization
- **Embeddings**: Token + segment + positional embeddings
- **MLM Head**: Masked Language Modeling head with weight tying
- **Attention**: Multi-head self-attention with proper masking

### Key Features
- Bidirectional encoder representations
- Masked Language Modeling (15% masking: 80% [MASK], 10% random, 10% unchanged)
- Pre-layer normalization for training stability
- Weight tying between input embeddings and MLM head
- Custom tokenizer with subword support

## ğŸ› ï¸ Usage

### Prerequisites
```bash
conda activate ml
pip install torch transformers pyyaml tqdm
```

### Quick Training Run
```bash
# Simple training with working parameters
python simple_train.py
```

### Full Training Pipeline
```bash
# Train BERT-Tiny model (now working!)
python train_mlm.py \
    --config configs/bert_tiny.yaml \
    --corpus data/toy_corpus.txt \
    --output_dir ./checkpoints \
    --seed 42
```

### Model Evaluation
```bash
# Evaluate trained model (note: some compatibility issues with dataset format)
python evaluate_mlm.py \
    --checkpoint checkpoints/final_model.pt \
    --corpus data/toy_corpus.txt \
    --vocab checkpoints/vocab.json \
    --batch_size 8
```

### Testing Individual Components
```bash
# Test tokenizer
python -m tokenization.simple_wp

# Test dataset creation
python -m tokenization.dataset_mlm

# Debug training issues
python debug_train.py
```

## ğŸ“Š Results

### Training Performance (BERT-Tiny)
- **Architecture**: d_model=128, n_layers=2, n_heads=2, d_ff=256
- **Vocabulary**: 147 tokens
- **Sequence Length**: 64 tokens
- **Parameters**: 309,651 trainable parameters

#### Training Results:
```
Epoch 1: Loss = 20.41, MLM Accuracy = 10.98%
Epoch 2: Loss = 13.97, MLM Accuracy = 22.15%
```

âœ… **Clear Convergence**: Loss decreased by 32%, accuracy doubled (2x improvement)

### Key Observations
- MLM loss decreases consistently showing the model learns to predict masked tokens
- MLM accuracy improves from random chance (~0.7%) to strong prediction (22%)
- Model successfully learns bidirectional representations from the toy corpus

## ğŸ”§ Configuration

### BERT-Tiny Config (`configs/bert_tiny.yaml`)
```yaml
# Model Architecture
vocab_size: 1000
d_model: 128
n_layers: 2
n_heads: 2
d_ff: 256
max_seq_length: 64
dropout: 0.1

# Training
batch_size: 8
num_epochs: 2
learning_rate: 3.0e-4
mlm_probability: 0.15
weight_tie_mlm: true
```

## ğŸ“ Project Structure

```
40_bert_intro/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layers.py              # Core transformer components
â”‚   â”œâ”€â”€ bert_encoder.py        # BERT encoder implementation  
â”‚   â””â”€â”€ heads.py               # MLM and NSP heads
â”œâ”€â”€ tokenization/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ simple_wp.py           # Simple tokenizer with subword support
â”‚   â””â”€â”€ dataset_mlm.py         # MLM dataset with masking logic
â”œâ”€â”€ data/
â”‚   â””â”€â”€ toy_corpus.txt         # Training corpus (63 sentences)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ bert_tiny.yaml         # Model configuration
â”œâ”€â”€ train_mlm.py               # Full training script
â”œâ”€â”€ simple_train.py            # Simplified working training
â”œâ”€â”€ evaluate_mlm.py            # Model evaluation
â”œâ”€â”€ debug_train.py             # Debugging utilities
â””â”€â”€ README.md
```

## ğŸ¯ Key Implementation Details

### Masked Language Modeling
- 15% of tokens are selected for masking
- 80% replaced with [MASK] token
- 10% replaced with random token  
- 10% kept unchanged
- Loss computed only on masked positions

### Transformer Architecture
- Pre-layer normalization (more stable than post-norm)
- Multi-head self-attention with proper padding masks
- GELU activation in feed-forward layers
- Residual connections around attention and FFN layers

### Embeddings
- **Token Embeddings**: Learned representations for each vocabulary token
- **Segment Embeddings**: Support for sentence pair tasks (though not used in MLM-only)
- **Position Embeddings**: Learned positional encodings (vs. sinusoidal)

### Training Stability
- Gradient clipping (max norm = 1.0)
- AdamW optimizer with weight decay
- Cosine learning rate schedule with warmup
- Custom collate function to handle sequence length consistency
- Robust sequence length management with assertions to prevent tensor size mismatches

## ğŸš€ Extensions & Improvements

### Implemented Features
- âœ… BERT encoder architecture
- âœ… Masked Language Modeling
- âœ… Custom tokenizer
- âœ… Working training pipeline
- âœ… Gradient clipping and optimization

### Potential Upgrades
- [ ] Next Sentence Prediction (NSP) or Sentence Order Prediction (SOP)
- [ ] Dynamic masking per epoch
- [ ] Mixed precision training (AMP)
- [ ] Rotary positional embeddings (RoPE)
- [ ] TensorBoard/Weights & Biases logging
- [ ] Model checkpointing and resuming
- [ ] Fine-tuning on downstream tasks (SST-2, AG-News)

## ğŸ§  Key Learnings

1. **Sequence Length Management**: Proper truncation and padding is critical for stable training
2. **Masking Strategy**: The 80/10/10 masking ratio is important for learning robust representations
3. **Architecture Choices**: Pre-layer norm improves training stability over post-layer norm
4. **Weight Tying**: Sharing weights between input embeddings and output projection reduces parameters
5. **Bidirectional Context**: BERT's bidirectional nature allows richer representations than unidirectional models

## ğŸ“š References
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) 