# BERT-Tiny Configuration for Fast Experimentation
# Designed for toy datasets and quick sanity checks

# Model Architecture
vocab_size: 1000  # Will be updated based on actual vocabulary
d_model: 128      # Hidden size
n_layers: 2       # Number of transformer layers
n_heads: 2        # Number of attention heads
d_ff: 256         # Feed-forward dimension
max_seq_length: 64   # Maximum sequence length
dropout: 0.1      # Dropout rate

# Embeddings
n_segments: 2     # Number of segment types (for NSP)
pad_token_id: 0   # Padding token ID
use_sinusoidal_pos: false  # Use learned positional embeddings

# Training Objectives
use_nsp: false    # Enable Next Sentence Prediction
use_sop: false    # Enable Sentence Order Prediction (alternative to NSP)
mlm_probability: 0.15  # Probability of masking tokens
weight_tie_mlm: true   # Tie MLM head weights with input embeddings

# Training Hyperparameters
batch_size: 8     # Training batch size
num_epochs: 2     # Number of training epochs
learning_rate: 3.0e-4  # Initial learning rate
weight_decay: 0.01     # AdamW weight decay
warmup_ratio: 0.05     # Warmup steps as ratio of total steps

# Vocabulary
min_frequency: 2  # Minimum token frequency for vocabulary

# Optimization
gradient_clip_norm: 1.0  # Gradient clipping max norm
adam_beta1: 0.9         # Adam beta1
adam_beta2: 0.999       # Adam beta2
adam_eps: 1.0e-8        # Adam epsilon

# Logging and Checkpointing
log_interval: 100       # Log every N steps
save_interval: 1000     # Save checkpoint every N steps
eval_interval: 500      # Evaluate every N steps (if eval set provided)

# Data Processing
short_seq_prob: 0.1     # Probability of generating short sequences
nsp_probability: 0.5    # Probability of positive NSP examples
