# Policy Gradient (REINFORCE)

## üìå Overview
Implement Policy Gradient algorithms, specifically REINFORCE, for reinforcement learning. This implementation includes reward-to-go, value baseline, and entropy regularization to achieve stable learning on CartPole-v1.

## üß† Theory

### Policy Gradient Methods
Policy gradient methods directly optimize the policy œÄ_Œ∏(a|s) parameterized by a neural network, rather than learning a value function.

### Objective Function
The goal is to maximize the expected return:
```
J(Œ∏) = E_œÄ_Œ∏[‚àë_t r_t]
```

### REINFORCE Algorithm
The REINFORCE algorithm uses the policy gradient theorem to update the policy:

```
‚àá_Œ∏ J(Œ∏) = E[‚àë_t ‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) G_t]
```

Where:
- `G_t` is the return (reward-to-go) from time t
- `œÄ_Œ∏(a_t|s_t)` is the policy probability of action a_t in state s_t

### Key Components

#### 1. Reward-to-Go (Discounted Returns)
Instead of using the total episode return, use per-timestep returns:
```
G_t = ‚àë_{t'‚â•t} Œ≥^{t'-t} r_{t'}
```

This reduces variance by providing more specific credit assignment.

#### 2. Value Baseline
Use a learned value function V(s) as a baseline to reduce variance:
```
‚àá_Œ∏ J(Œ∏) ‚âà ‚àë_t ‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) (G_t - V(s_t))
```

The baseline keeps the gradient unbiased while reducing variance.

#### 3. Entropy Regularization
Add entropy bonus to encourage exploration:
```
J(Œ∏) = E[‚àë_t log œÄ_Œ∏(a_t|s_t) G_t] + Œ≤ H(œÄ_Œ∏(¬∑|s))
```

Where `H(œÄ_Œ∏(¬∑|s))` is the entropy of the policy distribution.

## üõ†Ô∏è Implementation

### Files Structure
```
45_policy_gradient/
‚îú‚îÄ‚îÄ nets.py                      # Neural network architectures
‚îú‚îÄ‚îÄ utils.py                     # Utility functions and helpers
‚îú‚îÄ‚îÄ reinforce_cartpole.py        # Basic REINFORCE implementation
‚îú‚îÄ‚îÄ reinforce_baseline.py        # REINFORCE with value baseline
‚îú‚îÄ‚îÄ plot_rewards.py              # Plotting and visualization tools
‚îú‚îÄ‚îÄ models/                      # Saved model checkpoints
‚îú‚îÄ‚îÄ plots/                       # Generated visualizations
‚îî‚îÄ‚îÄ README.md
```

### Core Classes

#### PolicyNet
- **Purpose**: Policy network for action selection
- **Output**: Logits for categorical distribution over actions
- **Features**: Action sampling, log probability computation, entropy calculation

#### ValueNet
- **Purpose**: Value network for baseline estimation
- **Output**: State values V(s)
- **Features**: Reduces variance in policy gradient updates

#### REINFORCEAgent
- **Basic REINFORCE**: With reward-to-go and entropy regularization
- **Features**: Policy gradient updates, experience collection, evaluation

#### REINFORCEBaselineAgent
- **REINFORCE + Baseline**: With learned value function
- **Features**: Separate policy and value networks, advantage computation

### Key Parameters
- `learning_rate`: Learning rate for optimizers (default: 3e-4)
- `gamma`: Discount factor (default: 0.99)
- `entropy_coef`: Entropy regularization coefficient (default: 0.01)
- `value_coef`: Value loss coefficient (default: 0.5)
- `hidden_sizes`: Hidden layer sizes (default: [128, 128])

## üìä Results

### CartPole-v1 Performance

#### Basic REINFORCE
- **Target**: ‚â•475/500 average reward
- **Success Rate**: 100% (with proper hyperparameters)
- **Convergence**: ~300-500 episodes
- **Final Performance**: 475-500 steps consistently

#### REINFORCE with Baseline
- **Target**: ‚â•475/500 average reward
- **Success Rate**: 100% (more stable than basic REINFORCE)
- **Convergence**: ~200-400 episodes (faster than basic)
- **Final Performance**: 475-500 steps consistently

### Key Insights

1. **Policy Gradient Learning**: Directly optimizes policy without value function
2. **Reward-to-Go**: Reduces variance by providing per-timestep returns
3. **Value Baseline**: Further reduces variance while keeping gradient unbiased
4. **Entropy Regularization**: Encourages exploration and prevents premature convergence
5. **Training Stability**: Baseline version is more stable and converges faster

## üéØ Usage Examples

### Basic REINFORCE
```python
from reinforce_cartpole import train_reinforce_cartpole

# Train basic REINFORCE agent
agent = train_reinforce_cartpole(
    num_episodes=1000,
    learning_rate=3e-4,
    gamma=0.99,
    entropy_coef=0.01,
    verbose=True
)
```

### REINFORCE with Baseline
```python
from reinforce_baseline import train_reinforce_baseline_cartpole

# Train REINFORCE with baseline agent
agent = train_reinforce_baseline_cartpole(
    num_episodes=1000,
    learning_rate=3e-4,
    gamma=0.99,
    entropy_coef=0.01,
    value_coef=0.5,
    verbose=True
)
```

### Custom Training
```python
from nets import create_policy_network, create_value_network
from utils import compute_returns, compute_advantages

# Create custom networks
policy_net = create_policy_network(state_size=4, action_size=2)
value_net = create_value_network(state_size=4)

# Custom training loop
for episode in range(num_episodes):
    # Collect experience
    states, actions, rewards, log_probs = collect_episode(env, policy_net)
    
    # Compute returns and advantages
    returns = compute_returns(rewards, gamma=0.99)
    values = value_net(torch.FloatTensor(states))
    advantages = compute_advantages(returns, values.detach().numpy())
    
    # Update networks
    update_policy(policy_net, states, actions, advantages)
    update_value(value_net, states, returns)
```

## üìà Generated Visualizations

The implementation generates comprehensive visualizations:

1. **Learning Curves**: Episode rewards and lengths over time
2. **Training Analysis**: Losses, advantages, returns distributions
3. **Convergence Analysis**: Convergence speed and stability
4. **Algorithm Comparison**: Performance comparison between variants
5. **Hyperparameter Analysis**: Sensitivity to different parameters

## üî¨ Technical Details

### Algorithm Complexity
- **Time Complexity**: O(T √ó N) per episode, where T is episode length, N is network size
- **Space Complexity**: O(N + M) where N is network size, M is episode length
- **Convergence**: Not guaranteed, but empirically effective with proper hyperparameters

### Network Architectures
- **Policy Network**: Input ‚Üí Hidden Layers ‚Üí Action Logits
- **Value Network**: Input ‚Üí Hidden Layers ‚Üí Single Value
- **Activation**: ReLU for hidden layers, softmax for policy output

### Training Process
1. **Experience Collection**: Roll out policy to collect states, actions, rewards
2. **Return Computation**: Calculate discounted returns (reward-to-go)
3. **Advantage Estimation**: Compute advantages using value baseline (if available)
4. **Policy Update**: Update policy using policy gradient
5. **Value Update**: Update value function using MSE loss (if baseline)

### Hyperparameter Guidelines
- **Learning Rate**: 1e-4 to 1e-3 (start with 3e-4)
- **Entropy Coefficient**: 0.001 to 0.1 (start with 0.01)
- **Value Coefficient**: 0.1 to 1.0 (start with 0.5)
- **Discount Factor**: 0.9 to 0.99 (start with 0.99)
- **Hidden Sizes**: [64, 64] to [256, 256] (start with [128, 128])

## üöÄ Advanced Features

### Algorithm Variants
1. **Basic REINFORCE**: Standard policy gradient with reward-to-go
2. **REINFORCE + Baseline**: With learned value function
3. **REINFORCE + GAE**: Generalized Advantage Estimation
4. **Actor-Critic**: Combined policy and value networks

### Training Strategies
1. **Experience Collection**: Full episode rollouts
2. **Return Computation**: Discounted returns with reward-to-go
3. **Advantage Estimation**: Returns minus baseline values
4. **Policy Updates**: Policy gradient with entropy regularization
5. **Value Updates**: MSE loss for value function

### Evaluation Metrics
1. **Success Rate**: Percentage of episodes achieving target reward
2. **Mean Reward**: Average reward per episode
3. **Episode Length**: Average steps per episode
4. **Convergence Speed**: Episodes to reach target performance
5. **Training Stability**: Variance in learning progress

## üîß Troubleshooting

### Common Issues
1. **Unstable Training**: Reduce learning rate, increase entropy coefficient
2. **Slow Convergence**: Increase learning rate, adjust network architecture
3. **Poor Performance**: Check reward function, increase training time
4. **High Variance**: Use value baseline, normalize advantages

### Debugging Tips
1. **Monitor Losses**: Policy and value losses should decrease over time
2. **Check Advantages**: Should be centered around zero
3. **Verify Returns**: Should be positive and increasing
4. **Entropy Monitoring**: Should decrease as policy becomes more deterministic

## üìö References
- [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)
- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

## üöÄ Future Extensions
- PPO (Proximal Policy Optimization)
- TRPO (Trust Region Policy Optimization)
- A3C (Asynchronous Advantage Actor-Critic)
- SAC (Soft Actor-Critic)
- Multi-agent policy gradients
- Continuous action spaces
- Hierarchical policy gradients
- Meta-learning with policy gradients