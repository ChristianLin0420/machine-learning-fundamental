# Expectation-Maximization (EM) Algorithm

## ğŸ“Œ Overview
Implement the Expectation-Maximization algorithm for parameter estimation in probabilistic models with latent variables.

## ğŸ§  Key Concepts
- Latent Variable Models
- Expectation Step (E-step)
- Maximization Step (M-step)
- Lower Bound on Log-Likelihood

## ğŸ› ï¸ Implementation
- General EM framework
- E-step: Computing posterior probabilities
- M-step: Parameter updates
- Convergence monitoring and stopping criteria

## ğŸ“Š Results
- Log-likelihood convergence plots
- Parameter evolution during iterations
- Comparison with direct optimization methods
- Applications to various probabilistic models

## ğŸ“š References
- [EM Algorithm Tutorial](https://towardsdatascience.com/expectation-maximization-algorithm-clearly-explained-abb17a31b820)
- [Maximum Likelihood from Incomplete Data](https://web.mit.edu/6.435/www/Dempster77.pdf)
- [Machine Learning: A Probabilistic Perspective](https://probml.github.io/pml-book/) 