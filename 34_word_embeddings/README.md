# Word Embeddings (Word2Vec, GloVe)

## 📌 Overview
Implement word embedding techniques to convert text into dense vector representations that capture semantic relationships.

## 🧠 Key Concepts
- Distributed Representations
- Word2Vec (Skip-gram, CBOW)
- GloVe (Global Vectors)
- Semantic Similarity and Analogies

## 🛠️ Implementation
- Skip-gram model implementation
- Negative sampling technique
- GloVe objective function
- Vector similarity computations

## 📊 Results
- Word similarity visualizations (t-SNE)
- Analogy tasks (king - man + woman = queen)
- Embedding quality evaluation
- Comparison of different embedding methods

## 📚 References
- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)
- [GloVe Paper](https://nlp.stanford.edu/pubs/glove.pdf)
- [Word Embeddings Guide](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) 